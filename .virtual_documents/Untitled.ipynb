# ! pip install -r requirements.txt


import tarfile


LLAMA_RECIPES_TARBALL = "./llama_recipes.tar.gz"

def untar_llama_finetuning_recipe_tarball(tarball_path: str, target: str) -> None:
    """Untar the LLama Finetuning receipe repo."""
    with tarfile.open(tarball_path, "r") as llama_recipe_tar:
        llama_recipe_tar.extractall(target)

untar_llama_finetuning_recipe_tarball(tarball_path=LLAMA_RECIPES_TARBALL, target=".")


import fire

from llama_recipes.configs import train_config, fsdp_config
from llama_recipes.utils.config_utils import update_config


# I: Default training config
train_config()


# I: Default FSDP config
fsdp_config()


/opt/conda/bin/python3.10 
transfer_learning.py 
--add_input_output_demarcation_key True -> SM
--chat_dataset False - SM
--enable_fsdp True 
--epoch 1 
--instruction_tuned True -> SM
--int8_quantization False 
--learning_rate 0.0001 -> lr in train_config
--lora_alpha 32 -> PEFT
--lora_dropout 0.05 -> PEFT
--lora_r 8 -> PEFT
--max_input_length 2048 -> SM
--max_train_samples -1 -> SM
--max_val_samples -1 -> SM
--per_device_eval_batch_size 1 -> SM Finetuning using deepseed
--per_device_train_batch_size 2 -> PEFT
--preprocessing_num_workers None -> SM
--seed 10 
--target_modules q_proj,v_proj -> PEFT
--train_data_split_seed 0 -> SM
--validation_split_ratio 0.2 -> SM


def main(**kwargs) -> None:
    # Untar llama recipes tarball
    print(kwargs)
    update_config((train_config, fsdp_config), **kwargs)
    # Delete untarred llama recipes tarball
    return

# if __name__ == "__main__":
#     fire.Fire(main)


main(
    model_name="./models/CodeLlama-13b-Python-HF",
    dist_checkpoint_root_folder="./checkpoints/CodeLlama-13b-Python-HF",
    output_dir="./output/CodeLlama-13b-Python-HF",
    training_dataset="./dataset/train",
    validation_dataset="./dataset/validation",
    prompt_template="template.json",
    enable_fsdp=True,
    num_epochs=1,
    quantization=False, # int_8 Quantization
    learning_rate=0.001,
    seed=10,
)


# Create a folders called utils, config, constants


import os


os.path.exists("llama_recipe")


get_ipython().run_cell_magic("sh", "", """
python train.py \
    --model_dir ./models/CodeLlama-13b-Python-HF \
    --training_dataset ./dataset/train \
    --validation_dataset ./dataset/validation \
    --prompt_template template.json \
    --enable_fsdp True \
    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-13b-Python-HF \
    --num_epochs 1 \
    --int8_quantization False \
    --learning_rate 0.001 \
    --seed 10 \
    --use_peft True \
    --peft_output_dir ./output/CodeLlama-13b-Python-HF""")


# int_8 Quantization

# Retar the llama_finetuning folder


import torch

def get_num_gpus():
    if torch.cuda.is_available():
        return torch.cuda.device_count()
    else:
        return 0
    
get_num_gpus()



