# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./models/CodeLlama-13b-Python-HF")
model = AutoModelForCausalLM.from_pretrained("./models/CodeLlama-13b-Python-HF")


PROMPT = '''
def remove_non_ascii(s: str) -> str:
    """ <FILL_ME>
    return result
'''


input_ids = tokenizer(PROMPT, return_tensors="pt")["input_ids"]


generated_ids = model.generate(input_ids, max_new_tokens=128)


filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]


print(PROMPT.replace("<FILL_ME>", filling))
