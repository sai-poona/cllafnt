{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4bbee3f-2e95-4e28-8eba-b3663e32debf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be7c51-f949-4285-a6f8-c05228c306d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "/opt/conda/bin/python3.10 \n",
    "transfer_learning.py \n",
    "--add_input_output_demarcation_key True -> SM\n",
    "# --chat_dataset False - SM\n",
    "# --enable_fsdp True \n",
    "# --epoch 1 \n",
    "# --instruction_tuned True -> SM\n",
    "# --int8_quantization False \n",
    "# --learning_rate 0.0001 -> lr in train_config\n",
    "# --lora_alpha 32 -> PEFT\n",
    "# --lora_dropout 0.05 -> PEFT\n",
    "# --lora_r 8 -> PEFT\n",
    "# --max_input_length 2048 -> SM\n",
    "--max_train_samples -1 -> SM\n",
    "--max_val_samples -1 -> SM\n",
    "--per_device_eval_batch_size 1 -> SM Finetuning using deepseed\n",
    "--per_device_train_batch_size 2 -> PEFT\n",
    "--preprocessing_num_workers None -> SM\n",
    "# --seed 10 \n",
    "# --target_modules q_proj,v_proj -> PEFT\n",
    "--train_data_split_seed 0 -> SM\n",
    "--validation_split_ratio 0.2 -> SM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fa381-2d91-402e-98d0-bc5634028aeb",
   "metadata": {},
   "source": [
    "Note to self: \n",
    "- Loading a 13B Code Llama model requires instances bigger than g5.12xlarge and g4dn.12xlarge\n",
    "- Both have 4 GPU's 96GB GPU memory, 48vCPU's and 192GB CPU memory (RAM)\n",
    "- When loading the 13B pretrained model, the RAM is not sufficient. Need a bigger instance\n",
    "- If I force the model to load using GPU, by setting device_map=\"cuda\" in AutoModelForCausalLM.from_pretrained, I'm hitting GPU OOM errors\n",
    "    -  CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 47.06 MiB is free. \n",
    "- Using a 13B model for testing seems to be greedy. Switching to 7B models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e8ca6a-85e4-4646-8143-1b45db44d52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finetuning Args: Namespace(model_dir='./models/CodeLlama-7b-Python-HF', per_device_train_batch_size=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=1, num_workers_dataloader=1, learning_rate=0.001, weight_decay=0.0, gamma=0.85, seed=10, int8_quantization=False, freeze_layers=False, num_freeze_layers=1, use_fast_kernels=False, save_metrics=False, run_validation=True, val_batch_size=1, enable_fsdp=True, fsdp_checkpoint_root_dir='./checkpoints/CodeLlama-7b-Python-HF', low_cpu_fsdp=False, mixed_precision=True, use_fp16=False, pure_bf16=False, optimizer='AdamW', save_optimizer=False, use_peft=True, peft_method='lora', peft_output_dir='./output/CodeLlama-7b-Python-HF', lora_r=8, lora_alpha=32, lora_dropout=0.05, target_modules='q_proj,v_proj', train_dir='./data/train', validation_dir='./data/validation', file_extension='jsonl', prompt_template='./data/template.json', validation_split_ratio=0.2, max_input_length=-1, model_output_dir='finetuned_model')\n",
      "INFO:root:Executing command:\n",
      "INFO:root:['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-7b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '1', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--train_dir', './data/train', '--validation_dir', './data/validation', '--file_extension', 'jsonl', '--prompt_template', './data/template.json', '--validation_split_ratio', '0.2', '--max_input_length', '-1', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-7b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-7b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']\n",
      "[2024-04-01 11:31:16,403] torch.distributed.run: [WARNING] \n",
      "[2024-04-01 11:31:16,403] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-01 11:31:16,403] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-04-01 11:31:16,403] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 1. Rank is 1. World Size is 4\n",
      "INFO:root:Setting torch device = 1\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 2. Rank is 2. World Size is 4\n",
      "INFO:root:Setting torch device = 2\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 0. Rank is 0. World Size is 4\n",
      "INFO:root:Setting torch device = 0\n",
      "INFO:root:Loading the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running with torch dist debug set to detail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 3. Rank is 3. World Size is 4\n",
      "INFO:root:Setting torch device = 3\n",
      "INFO:root:Loading the tokenizer.\n",
      "INFO:root:Loading the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Loading the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:--> Training Set Length = 4954\n",
      "INFO:root:--> Validation Set Length = 4\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.80s/it]\n",
      "INFO:root:Printing Model Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model ./models/CodeLlama-7b-Python-HF\n",
      "\n",
      "--> ./models/CodeLlama-7b-Python-HF has 6738.415616 Million params\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bFloat16 enabled for mixed precision - using bfSixteen policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.81s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.81s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n",
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/309 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/309 [00:00<?, ?it/s]INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/309 [00:00<?, ?it/s]INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1/1, step 308/309 completed (loss: 0.3123700022697449): 100%|\u001b[34m██████████\u001b[0m| 309/309 [1:06:44<00:00, 12.96s/it]]\n",
      "Training Epoch: 1/1, step 308/309 completed (loss: 0.39486873149871826): 100%|\u001b[34m██████████\u001b[0m| 309/309 [1:06:44<00:00, 12.96s/it]\n",
      "Training Epoch: 1/1, step 308/309 completed (loss: 0.3212045431137085): 100%|\u001b[34m██████████\u001b[0m| 309/309 [1:06:43<00:00, 12.96s/it]\n",
      "Training Epoch: 1/1, step 308/309 completed (loss: 0.3712836503982544): 100%|\u001b[34m██████████\u001b[0m| 309/309 [1:06:44<00:00, 12.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 1\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.89s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.89s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.89s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.3460, device='cuda:0') eval_epoch_loss=tensor(0.2971, device='cuda:0')\n",
      "we are about to save the PEFT modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory\n",
      "best eval loss on epoch 1 is 0.29714274406433105\n",
      "Epoch 1: train_perplexity=1.5228, train_epoch_loss=0.4206, epoch time 4004.818402315s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Key: avg_train_prep, Value: 1.5227992534637451\n",
      "INFO:root:Key: avg_train_loss, Value: 0.42055025696754456\n",
      "INFO:root:Key: avg_eval_prep, Value: 1.3460074663162231\n",
      "INFO:root:Key: avg_eval_loss, Value: 0.29714274406433105\n",
      "INFO:root:Key: avg_epoch_time, Value: 4004.818402315\n",
      "INFO:root:Key: avg_checkpoint_time, Value: 8.128523784000208\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/cllafnt/train.py\", line 192, in <module>\n",
      "    fire.Fire(main)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/cllafnt/train.py\", line 182, in main\n",
      "    base_model_dir=finetuning_args.model_name,\n",
      "AttributeError: 'Namespace' object has no attribute 'model_name'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\npython train.py \\\\\\n    --model_dir ./models/CodeLlama-7b-Python-HF \\\\\\n    --enable_fsdp True \\\\\\n    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \\\\\\n    --num_epochs 1 \\\\\\n    --int8_quantization False \\\\\\n    --learning_rate 0.001 \\\\\\n    --seed 10 \\\\\\n    --use_peft True \\\\\\n    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\\\\n    --train_dir ./data/train \\\\\\n    --validation_dir ./data/validation \\\\\\n    --file_extension jsonl \\\\\\n    --prompt_template ./data/template.json\\n    \\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython train.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --model_dir ./models/CodeLlama-7b-Python-HF \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --enable_fsdp True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --num_epochs 1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --int8_quantization False \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --learning_rate 0.001 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --seed 10 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --use_peft True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --peft_output_dir ./output/CodeLlama-7b-Python-HF \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --train_dir ./data/train \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --validation_dir ./data/validation \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --file_extension jsonl \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --prompt_template ./data/template.json\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\npython train.py \\\\\\n    --model_dir ./models/CodeLlama-7b-Python-HF \\\\\\n    --enable_fsdp True \\\\\\n    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \\\\\\n    --num_epochs 1 \\\\\\n    --int8_quantization False \\\\\\n    --learning_rate 0.001 \\\\\\n    --seed 10 \\\\\\n    --use_peft True \\\\\\n    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\\\\n    --train_dir ./data/train \\\\\\n    --validation_dir ./data/validation \\\\\\n    --file_extension jsonl \\\\\\n    --prompt_template ./data/template.json\\n    \\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python train.py \\\n",
    "    --model_dir ./models/CodeLlama-7b-Python-HF \\\n",
    "    --enable_fsdp True \\\n",
    "    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \\\n",
    "    --num_epochs 1 \\\n",
    "    --int8_quantization False \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --seed 10 \\\n",
    "    --use_peft True \\\n",
    "    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\n",
    "    --train_dir ./data/train \\\n",
    "    --validation_dir ./data/validation \\\n",
    "    --file_extension jsonl \\\n",
    "    --prompt_template ./data/template.json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a33fc7b-db63-4610-a772-299dd6439447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrying the above command to save peft model as I made a mistake of passing incorrect arg parameter for base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cce4cff-9d89-444f-bfdb-3d49dada3256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finetuning Args: Namespace(model_dir='./models/CodeLlama-7b-Python-HF', per_device_train_batch_size=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=1, num_workers_dataloader=1, learning_rate=0.001, weight_decay=0.0, gamma=0.85, seed=10, int8_quantization=False, freeze_layers=False, num_freeze_layers=1, use_fast_kernels=False, save_metrics=False, run_validation=True, val_batch_size=1, enable_fsdp=True, fsdp_checkpoint_root_dir='./checkpoints/CodeLlama-7b-Python-HF', low_cpu_fsdp=False, mixed_precision=True, use_fp16=False, pure_bf16=False, optimizer='AdamW', save_optimizer=False, use_peft=True, peft_method='lora', peft_output_dir='./output/CodeLlama-7b-Python-HF', lora_r=8, lora_alpha=32, lora_dropout=0.05, target_modules='q_proj,v_proj', train_dir='./data/train', validation_dir='./data/validation', file_extension='jsonl', prompt_template='./data/template.json', validation_split_ratio=0.2, max_input_length=-1, model_output_dir='finetuned_model')\n",
      "INFO:root:Executing command:\n",
      "INFO:root:['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-7b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '1', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--train_dir', './data/train', '--validation_dir', './data/validation', '--file_extension', 'jsonl', '--prompt_template', './data/template.json', '--validation_split_ratio', '0.2', '--max_input_length', '-1', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-7b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-7b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']\n",
      "INFO:root:Combining pre-trained base model with the PEFT adapter module.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.04it/s]\n",
      "INFO:root:Saving the combined model in safetensors format.\n",
      "INFO:root:Saving complete.\n",
      "INFO:root:Saving the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Saving complete.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python train.py \\\n",
    "    --model_dir ./models/CodeLlama-7b-Python-HF \\\n",
    "    --enable_fsdp True \\\n",
    "    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \\\n",
    "    --num_epochs 1 \\\n",
    "    --int8_quantization False \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --seed 10 \\\n",
    "    --use_peft True \\\n",
    "    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\n",
    "    --train_dir ./data/train \\\n",
    "    --validation_dir ./data/validation \\\n",
    "    --file_extension jsonl \\\n",
    "    --prompt_template ./data/template.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d9f0a-fd19-40ed-868d-098e1e74b8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
