{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4bbee3f-2e95-4e28-8eba-b3663e32debf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f83131-6321-4d12-b73d-3341d99f20d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "\n",
    "LLAMA_RECIPES_TARBALL = \"./llama_recipes.tar.gz\"\n",
    "\n",
    "def untar_llama_finetuning_recipe_tarball(tarball_path: str, target: str) -> None:\n",
    "    \"\"\"Untar the LLama Finetuning receipe repo.\"\"\"\n",
    "    with tarfile.open(tarball_path, \"r\") as llama_recipe_tar:\n",
    "        llama_recipe_tar.extractall(target)\n",
    "\n",
    "untar_llama_finetuning_recipe_tarball(tarball_path=LLAMA_RECIPES_TARBALL, target=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778ec331-ad87-4dc0-9ce1-3aea3510c6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fire\n",
    "\n",
    "from llama_recipes.configs import train_config, fsdp_config\n",
    "from llama_recipes.utils.config_utils import update_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8e72f4-aff9-488c-a582-adcb5dbfb924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_config(model_name='PATH/to/LLAMA/7B', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=3, num_workers_dataloader=1, lr=0.0001, weight_decay=0.0, gamma=0.85, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='PATH/to/save/PEFT/model', freeze_layers=False, num_freeze_layers=1, quantization=False, one_gpu=False, save_model=True, dist_checkpoint_root_folder='PATH/to/save/FSDP/model', dist_checkpoint_folder='fine-tuned', save_optimizer=False, use_fast_kernels=False, use_wandb=False, save_metrics=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I: Default training config\n",
    "train_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5edf974-ca1b-497d-9216-58e98b33a330",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fsdp_config(mixed_precision=True, use_fp16=False, sharding_strategy=<ShardingStrategy.FULL_SHARD: 1>, hsdp=False, sharding_group_size=0, replica_group_size=0, checkpoint_type=<StateDictType.SHARDED_STATE_DICT: 3>, fsdp_activation_checkpointing=True, fsdp_cpu_offload=False, pure_bf16=False, optimizer='AdamW')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I: Default FSDP config\n",
    "fsdp_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be7c51-f949-4285-a6f8-c05228c306d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "/opt/conda/bin/python3.10 \n",
    "transfer_learning.py \n",
    "--add_input_output_demarcation_key True -> SM\n",
    "--chat_dataset False - SM\n",
    "--enable_fsdp True \n",
    "--epoch 1 \n",
    "--instruction_tuned True -> SM\n",
    "--int8_quantization False \n",
    "--learning_rate 0.0001 -> lr in train_config\n",
    "--lora_alpha 32 -> PEFT\n",
    "--lora_dropout 0.05 -> PEFT\n",
    "--lora_r 8 -> PEFT\n",
    "--max_input_length 2048 -> SM\n",
    "--max_train_samples -1 -> SM\n",
    "--max_val_samples -1 -> SM\n",
    "--per_device_eval_batch_size 1 -> SM Finetuning using deepseed\n",
    "--per_device_train_batch_size 2 -> PEFT\n",
    "--preprocessing_num_workers None -> SM\n",
    "--seed 10 \n",
    "--target_modules q_proj,v_proj -> PEFT\n",
    "--train_data_split_seed 0 -> SM\n",
    "--validation_split_ratio 0.2 -> SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987b9166-73dc-4c84-b90f-2ce15fe7b51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(**kwargs) -> None:\n",
    "    # Untar llama recipes tarball\n",
    "    print(kwargs)\n",
    "    update_config((train_config, fsdp_config), **kwargs)\n",
    "    # Delete untarred llama recipes tarball\n",
    "    return\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1048d19b-db96-4546-8471-7d675c52d39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': './models/CodeLlama-13b-Python-HF', 'dist_checkpoint_root_folder': './checkpoints/CodeLlama-13b-Python-HF', 'output_dir': './output/CodeLlama-13b-Python-HF', 'training_dataset': './dataset/train', 'validation_dataset': './dataset/validation', 'prompt_template': 'template.json', 'enable_fsdp': True, 'num_epochs': 1, 'quantization': False, 'learning_rate': 0.001, 'seed': 10}\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_name=\"./models/CodeLlama-13b-Python-HF\",\n",
    "    dist_checkpoint_root_folder=\"./checkpoints/CodeLlama-13b-Python-HF\",\n",
    "    output_dir=\"./output/CodeLlama-13b-Python-HF\",\n",
    "    training_dataset=\"./dataset/train\",\n",
    "    validation_dataset=\"./dataset/validation\",\n",
    "    prompt_template=\"template.json\",\n",
    "    enable_fsdp=True,\n",
    "    num_epochs=1,\n",
    "    quantization=False, # int_8 Quantization\n",
    "    learning_rate=0.001,\n",
    "    seed=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0e520-671f-43c3-b916-e2b5bee75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folders called utils, config, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6feab766-1c03-43a6-a5d0-790809b6349d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "053bdf76-548b-4741-bb39-757d2632c625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"llama_recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2e8ca6a-85e4-4646-8143-1b45db44d52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executing command:\n",
      "INFO:root:['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-13b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '1', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-13b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-13b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']\n",
      "[2024-03-31 05:44:11,137] torch.distributed.run: [WARNING] \n",
      "[2024-03-31 05:44:11,137] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-03-31 05:44:11,137] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-03-31 05:44:11,137] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/ec2-user/SageMaker/finetuning.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/finetuning.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/finetuning.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/finetuning.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 0. Rank is 0. World Size is 4\n",
      "INFO:root:Setting torch device = 0\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-13b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running with torch dist debug set to detail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 2. Rank is 2. World Size is 4\n",
      "INFO:root:Setting torch device = 2\n",
      "INFO:root:Local rank is 3. Rank is 3. World Size is 4\n",
      "INFO:root:Setting torch device = 3\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 1. Rank is 1. World Size is 4\n",
      "INFO:root:Setting torch device = 1\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-13b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-13b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-13b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "Loading checkpoint shards:  67%|██████▋   | 4/6 [24:34<15:48, 474.50s/it][2024-03-31 06:17:33,117] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 15115 closing signal SIGTERM\n",
      "[2024-03-31 06:17:33,119] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 15116 closing signal SIGTERM\n",
      "[2024-03-31 06:17:33,119] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 15118 closing signal SIGTERM\n",
      "[2024-03-31 06:17:35,549] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -9) local_rank: 2 (pid: 15117) of binary: /home/ec2-user/anaconda3/envs/pytorch_p310/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "finetuning.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-03-31_06:17:33\n",
      "  host      : ip-172-16-28-182.ap-southeast-2.compute.internal\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : -9 (pid: 15117)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 9 (SIGKILL) received by PID 15117\n",
      "============================================================\n",
      "ERROR:root:Subprocess script failed with return code: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/utils/base.py\", line 17, in run_with_error_handling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_dir='./models/CodeLlama-13b-Python-HF', per_device_train_batch_size=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=1, num_workers_dataloader=1, learning_rate=0.001, weight_decay=0.0, gamma=0.85, seed=10, int8_quantization=False, freeze_layers=False, num_freeze_layers=1, use_fast_kernels=False, save_metrics=False, run_validation=True, val_batch_size=1, enable_fsdp=True, fsdp_checkpoint_root_dir='./checkpoints/CodeLlama-13b-Python-HF', low_cpu_fsdp=False, mixed_precision=True, use_fp16=False, pure_bf16=False, optimizer='AdamW', save_optimizer=False, use_peft=True, peft_method='lora', peft_output_dir='./output/CodeLlama-13b-Python-HF', lora_r=8, lora_alpha=32, lora_dropout=0.05, target_modules='q_proj,v_proj')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    subprocess.run(command, shell=shell, check=True)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/subprocess.py\", line 526, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-13b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '1', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-13b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-13b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']' returned non-zero exit status 1.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train.py\", line 155, in <module>\n",
      "    fire.Fire(main)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/train.py\", line 149, in main\n",
      "    run_with_error_handling(command)\n",
      "  File \"/home/ec2-user/SageMaker/utils/base.py\", line 20, in run_with_error_handling\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: Command '['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-13b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '1', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-13b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-13b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\npython train.py \\\\\\n    --model_dir ./models/CodeLlama-13b-Python-HF \\\\\\n    --training_dataset ./dataset/train \\\\\\n    --validation_dataset ./dataset/validation \\\\\\n    --prompt_template template.json \\\\\\n    --enable_fsdp True \\\\\\n    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-13b-Python-HF \\\\\\n    --num_epochs 1 \\\\\\n    --int8_quantization False \\\\\\n    --learning_rate 0.001 \\\\\\n    --seed 10 \\\\\\n    --use_peft True \\\\\\n    --peft_output_dir ./output/CodeLlama-13b-Python-HF\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython train.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --model_dir ./models/CodeLlama-13b-Python-HF \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --training_dataset ./dataset/train \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --validation_dataset ./dataset/validation \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --prompt_template template.json \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --enable_fsdp True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-13b-Python-HF \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --num_epochs 1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --int8_quantization False \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --learning_rate 0.001 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --seed 10 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --use_peft True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --peft_output_dir ./output/CodeLlama-13b-Python-HF\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\npython train.py \\\\\\n    --model_dir ./models/CodeLlama-13b-Python-HF \\\\\\n    --training_dataset ./dataset/train \\\\\\n    --validation_dataset ./dataset/validation \\\\\\n    --prompt_template template.json \\\\\\n    --enable_fsdp True \\\\\\n    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-13b-Python-HF \\\\\\n    --num_epochs 1 \\\\\\n    --int8_quantization False \\\\\\n    --learning_rate 0.001 \\\\\\n    --seed 10 \\\\\\n    --use_peft True \\\\\\n    --peft_output_dir ./output/CodeLlama-13b-Python-HF\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python train.py \\\n",
    "    --model_dir ./models/CodeLlama-13b-Python-HF \\\n",
    "    --training_dataset ./dataset/train \\\n",
    "    --validation_dataset ./dataset/validation \\\n",
    "    --prompt_template template.json \\\n",
    "    --enable_fsdp True \\\n",
    "    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-13b-Python-HF \\\n",
    "    --num_epochs 1 \\\n",
    "    --int8_quantization False \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --seed 10 \\\n",
    "    --use_peft True \\\n",
    "    --peft_output_dir ./output/CodeLlama-13b-Python-HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33fc7b-db63-4610-a772-299dd6439447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_8 Quantization\n",
    "\n",
    "# Retar the llama_finetuning folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ae4d2f0-b922-4b73-a771-b0c0cb614987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_num_gpus():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.device_count()\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "get_num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce4cff-9d89-444f-bfdb-3d49dada3256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
