Uploaded ._utils_2024-04-05-03-48-10.tar.gz to S3 bucket s3://sandbox-dump/codellama-test/finetuned_models
Success notification sent via email!
0
Uploaded ._finetuned_model_2024-04-05-03-55-24.tar.gz to S3 bucket s3://sandbox-dump/codellama-test/finetuned_models
Success notification sent via email!
0
INFO:root:Finetuning Args: Namespace(model_dir='./models/CodeLlama-7b-Python-HF', per_device_train_batch_size=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=5, num_workers_dataloader=1, learning_rate=0.001, weight_decay=0.0, gamma=0.85, seed=10, int8_quantization=False, freeze_layers=False, num_freeze_layers=1, use_fast_kernels=False, save_metrics=False, run_validation=True, val_batch_size=1, enable_fsdp=True, fsdp_checkpoint_root_dir='./checkpoints/CodeLlama-7b-Python-HF', low_cpu_fsdp=False, mixed_precision=True, use_fp16=False, pure_bf16=False, optimizer='AdamW', save_optimizer=False, use_peft=True, peft_method='lora', peft_output_dir='./output/CodeLlama-7b-Python-HF', lora_r=8, lora_alpha=32, lora_dropout=0.05, target_modules='q_proj,v_proj', train_dir='./data/vivek/train', validation_dir='./data/vivek/validation', file_extension='jsonl', prompt_template='./data/vivek/template.json', validation_split_ratio=0.2, max_input_length=-1, model_output_dir='./finetuned_model/CodeLlama-7b-Python-HF/run1')
INFO:root:Executing command:
INFO:root:['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-7b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '5', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--train_dir', './data/vivek/train', '--validation_dir', './data/vivek/validation', '--file_extension', 'jsonl', '--prompt_template', './data/vivek/template.json', '--validation_split_ratio', '0.2', '--max_input_length', '-1', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-7b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-7b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']
[2024-04-05 04:44:34,063] torch.distributed.run: [WARNING] 
[2024-04-05 04:44:34,063] torch.distributed.run: [WARNING] *****************************************
[2024-04-05 04:44:34,063] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-05 04:44:34,063] torch.distributed.run: [WARNING] *****************************************
/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging
/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging
/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging
/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging
[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
INFO:root:Local rank is 2. Rank is 2. World Size is 4
INFO:root:Setting torch device = 2
INFO:root:Local rank is 0. Rank is 0. World Size is 4
INFO:root:Setting torch device = 0
--> Running with torch dist debug set to detail
INFO:root:Loading the tokenizer.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
INFO:root:Local rank is 3. Rank is 3. World Size is 4
INFO:root:Setting torch device = 3
INFO:root:Local rank is 1. Rank is 1. World Size is 4
INFO:root:Setting torch device = 1
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 90 examples [00:00, 342.04 examples/s]Generating train split: 90 examples [00:00, 341.77 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 10 examples [00:00, 9410.60 examples/s]
Map:   0%|                                                                                         | 0/90 [00:00<?, ? examples/s]Map: 100%|██████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 7172.48 examples/s]
Map:   0%|                                                                                         | 0/90 [00:00<?, ? examples/s]INFO:root:Loading the tokenizer.
INFO:root:Loading the tokenizer.
INFO:root:Loading the tokenizer.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Map: 100%|███████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 774.48 examples/s]Map: 100%|███████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 759.36 examples/s]
INFO:root:Using the default value of max_input_length=2048.
Map:   0%|                                                                                         | 0/90 [00:00<?, ? examples/s]Map: 100%|███████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 584.71 examples/s]Map: 100%|███████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 575.99 examples/s]
Map:   0%|                                                                                         | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3646.90 examples/s]
Map:   0%|                                                                                         | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1207.41 examples/s]
Map:   0%|                                                                                         | 0/10 [00:00<?, ? examples/s]Map: 100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 762.38 examples/s]
INFO:root:--> Training Set Length = 68
INFO:root:--> Validation Set Length = 4
INFO:root:Loading the pre-trained model and setup its configuration
INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF
INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False
INFO:root:Using the default value of max_input_length=2048.
INFO:root:Using the default value of max_input_length=2048.
INFO:root:Using the default value of max_input_length=2048.
INFO:root:Loading the pre-trained model and setup its configuration
INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF
INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False
INFO:root:Loading the pre-trained model and setup its configuration
INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF
INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False
INFO:root:Loading the pre-trained model and setup its configuration
INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF
INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False
Loading checkpoint shards:   0%|                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|█████████████████████▋                                           | 1/3 [18:13<36:27, 1093.84s/it]Loading checkpoint shards:  33%|█████████████████████▋                                           | 1/3 [18:14<36:28, 1094.19s/it]Loading checkpoint shards:  33%|█████████████████████▋                                           | 1/3 [18:14<36:28, 1094.20s/it]Loading checkpoint shards:  33%|█████████████████████▋                                           | 1/3 [18:14<36:28, 1094.20s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████▎                     | 2/3 [37:05<18:35, 1115.86s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████▎                     | 2/3 [37:05<18:35, 1116.00s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████▎                     | 2/3 [37:05<18:36, 1116.06s/it]Loading checkpoint shards:  67%|███████████████████████████████████████████▎                     | 2/3 [37:05<18:35, 1116.00s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 969.11s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 1006.59s/it]
INFO:root:Printing Model Size
INFO:root:Using PEFT
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
INFO:root:Setting up FSDP if enable_fsdp is enabled
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 969.16s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 1006.63s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 969.20s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 1006.63s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 969.20s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 3/3 [50:19<00:00, 1006.63s/it]
INFO:root:Printing Model Size
INFO:root:Using PEFT
INFO:root:Printing Model Size
INFO:root:Using PEFT
INFO:root:Printing Model Size
--> Model ./models/CodeLlama-7b-Python-HF

--> ./models/CodeLlama-7b-Python-HF has 6738.415616 Million params

INFO:root:Using PEFT
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
INFO:root:Setting up FSDP if enable_fsdp is enabled
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
INFO:root:Setting up FSDP if enable_fsdp is enabled
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
INFO:root:Setting up FSDP if enable_fsdp is enabled
bFloat16 enabled for mixed precision - using bfSixteen policy
--> applying fsdp activation checkpointing...
INFO:root:Initializing the optimizer and learning rate scheduler
INFO:root:Starting the training process
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0m--> applying fsdp activation checkpointing...
INFO:root:Initializing the optimizer and learning rate scheduler
INFO:root:Starting the training process
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0m--> applying fsdp activation checkpointing...
--> applying fsdp activation checkpointing...
INFO:root:Initializing the optimizer and learning rate scheduler
INFO:root:Starting the training process
INFO:root:Initializing the optimizer and learning rate scheduler
INFO:root:Starting the training process
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0m/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training Epoch: 1:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:18<00:54, 18.17s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 0.9405117630958557):  25%|[34m██████▌                   [0m| 1/4 [00:18<00:54, 18.17s/it][0mTraining Epoch: 1:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:18<00:55, 18.60s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 1.1103975772857666):  25%|[34m██████▌                   [0m| 1/4 [00:18<00:55, 18.60s/it][0mTraining Epoch: 1:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:18<00:55, 18.39s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 0.6011255383491516):  25%|[34m██████▌                   [0m| 1/4 [00:18<00:55, 18.39s/it][0mTraining Epoch: 1:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:18<00:54, 18.14s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 1.0878700017929077):  25%|[34m██████▌                   [0m| 1/4 [00:18<00:54, 18.14s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 0.6011255383491516):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.17s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.6375301480293274):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.17s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 0.9405117630958557):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.08s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 1.1103975772857666):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.26s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.5719085931777954):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.08s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.6602030396461487):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.26s/it][0mTraining Epoch: 1/5, step 0/4 completed (loss: 1.0878700017929077):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.07s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.6133289933204651):  50%|[34m█████████████             [0m| 2/4 [00:31<00:30, 15.07s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.5719085931777954):  75%|[34m███████████████████▌      [0m| 3/4 [00:43<00:14, 14.08s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.6602030396461487):  75%|[34m███████████████████▌      [0m| 3/4 [00:44<00:14, 14.17s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.5636650919914246):  75%|[34m███████████████████▌      [0m| 3/4 [00:43<00:14, 14.08s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.6375301480293274):  75%|[34m███████████████████▌      [0m| 3/4 [00:44<00:14, 14.13s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.5650815367698669):  75%|[34m███████████████████▌      [0m| 3/4 [00:44<00:14, 14.17s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.9845448136329651):  75%|[34m███████████████████▌      [0m| 3/4 [00:44<00:14, 14.13s/it][0mTraining Epoch: 1/5, step 1/4 completed (loss: 0.6133289933204651):  75%|[34m███████████████████▌      [0m| 3/4 [00:43<00:14, 14.07s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.7090854048728943):  75%|[34m███████████████████▌      [0m| 3/4 [00:43<00:14, 14.07s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.5636650919914246): 100%|[34m██████████████████████████[0m| 4/4 [00:56<00:00, 13.62s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.9845448136329651): 100%|[34m██████████████████████████[0m| 4/4 [00:57<00:00, 13.65s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.5650815367698669): 100%|[34m██████████████████████████[0m| 4/4 [00:57<00:00, 13.68s/it][0mTraining Epoch: 1/5, step 3/4 completed (loss: 0.5114607810974121): 100%|[34m██████████████████████████[0m| 4/4 [00:57<00:00, 13.65s/it][0mTraining Epoch: 1/5, step 3/4 completed (loss: 0.5256215929985046): 100%|[34m██████████████████████████[0m| 4/4 [00:56<00:00, 13.62s/it][0mTraining Epoch: 1/5, step 3/4 completed (loss: 0.4633955955505371): 100%|[34m██████████████████████████[0m| 4/4 [00:57<00:00, 13.68s/it][0mTraining Epoch: 1/5, step 2/4 completed (loss: 0.7090854048728943): 100%|[34m██████████████████████████[0m| 4/4 [00:56<00:00, 13.62s/it][0mTraining Epoch: 1/5, step 3/4 completed (loss: 0.5533013343811035): 100%|[34m██████████████████████████[0m| 4/4 [00:56<00:00, 13.62s/it][0mTraining Epoch: 1/5, step 3/4 completed (loss: 0.5256215929985046): 100%|[34m██████████████████████████[0m| 4/4 [00:56<00:00, 14.23s/it][0m
Training Epoch: 1/5, step 3/4 completed (loss: 0.5114607810974121): 100%|[34m██████████████████████████[0m| 4/4 [00:57<00:00, 14.29s/it][0m
Training Epoch: 1/5, step 3/4 completed (loss: 0.4633955955505371): 100%|[34m██████████████████████████[0m| 4/4 [00:57<00:00, 14.34s/it][0m
Training Epoch: 1/5, step 3/4 completed (loss: 0.5533013343811035): 100%|[34m██████████████████████████[0m| 4/4 [00:56<00:00, 14.23s/it][0m
Max CUDA memory allocated was 18 GB
Max CUDA memory reserved was 20 GB
Peak active CUDA memory was 18 GB
CUDA Malloc retries : 1
CPU Total Peak Memory consumed during the train (max): 2 GB
evaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.05s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.05s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.08s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.08s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.09s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.09s/it][0m
 eval_ppl=tensor(1.9567, device='cuda:0') eval_epoch_loss=tensor(0.6713, device='cuda:0')
we are about to save the PEFT modules
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.
  warnings.warn(
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.
  warnings.warn(
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.
  warnings.warn(
/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.
  warnings.warn(
PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory
best eval loss on epoch 1 is 0.6712804436683655
Epoch 1: train_perplexity=2.0011, train_epoch_loss=0.6937, epoch time 57.614537770000425s
Training Epoch: 2:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 2:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 2:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 2:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training Epoch: 2:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 0.7796167731285095):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 0.4057365655899048):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 0.8235359191894531):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 1.002051830291748):  25%|[34m██████▊                    [0m| 1/4 [00:14<00:42, 14.01s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 0.4057365655899048):  50%|[34m█████████████             [0m| 2/4 [00:26<00:26, 13.35s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.46164485812187195):  50%|[34m████████████▌            [0m| 2/4 [00:26<00:26, 13.35s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 0.8235359191894531):  50%|[34m█████████████             [0m| 2/4 [00:26<00:26, 13.36s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.4394163489341736):  50%|[34m█████████████             [0m| 2/4 [00:26<00:26, 13.36s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 1.002051830291748):  50%|[34m█████████████▌             [0m| 2/4 [00:26<00:26, 13.36s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.4687763452529907):  50%|[34m█████████████             [0m| 2/4 [00:26<00:26, 13.36s/it][0mTraining Epoch: 2/5, step 0/4 completed (loss: 0.7796167731285095):  50%|[34m█████████████             [0m| 2/4 [00:26<00:26, 13.36s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.4050236940383911):  50%|[34m█████████████             [0m| 2/4 [00:26<00:26, 13.36s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.46164485812187195):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.9986544847488403):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.4394163489341736):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.5306172370910645):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.4687763452529907):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 1/4 completed (loss: 0.4050236940383911):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.4352325201034546):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.40914270281791687):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.16s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.4352325201034546): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 3/4 completed (loss: 0.3423854112625122): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.40914270281791687): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.5306172370910645): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 2/4 completed (loss: 0.9986544847488403): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 3/4 completed (loss: 0.3935103118419647): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 3/4 completed (loss: 0.37439751625061035): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 3/4 completed (loss: 0.4187711775302887): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.07s/it][0mTraining Epoch: 2/5, step 3/4 completed (loss: 0.3423854112625122): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.20s/it][0m
Training Epoch: 2/5, step 3/4 completed (loss: 0.3935103118419647): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.20s/it][0m
Training Epoch: 2/5, step 3/4 completed (loss: 0.37439751625061035): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.20s/it][0m
Training Epoch: 2/5, step 3/4 completed (loss: 0.4187711775302887): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.20s/it][0m
Max CUDA memory allocated was 18 GB
Max CUDA memory reserved was 20 GB
Peak active CUDA memory was 18 GB
CUDA Malloc retries : 101
CPU Total Peak Memory consumed during the train (max): 2 GB
evaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.87s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.90s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0m
 eval_ppl=tensor(1.8413, device='cuda:0') eval_epoch_loss=tensor(0.6105, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory
best eval loss on epoch 2 is 0.6104682683944702
Epoch 2: train_perplexity=1.7212, train_epoch_loss=0.5430, epoch time 53.77026554599979s
Training Epoch: 3:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 3:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 3:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 3:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training Epoch: 3:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.6926034092903137):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.7837359309196472):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.9934172630310059):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.29728344082832336):  25%|[34m██████▎                  [0m| 1/4 [00:14<00:42, 14.13s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.6926034092903137):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.32346275448799133):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.9934172630310059):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.37824520468711853):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.29728344082832336):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 0/4 completed (loss: 0.7837359309196472):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.3844747841358185):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.3506670892238617):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.3844747841358185):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.37824520468711853):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.8718414902687073):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.37782981991767883):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.32346275448799133):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.3384452760219574):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 1/4 completed (loss: 0.3506670892238617):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.46573472023010254):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.18s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.46573472023010254): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 3/4 completed (loss: 0.3539196252822876): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.37782981991767883): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 3/4 completed (loss: 0.2881513833999634): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.8718414902687073): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 3/4 completed (loss: 0.31459495425224304): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 2/4 completed (loss: 0.3384452760219574): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 3/4 completed (loss: 0.33240947127342224): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 3/5, step 3/4 completed (loss: 0.2881513833999634): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.23s/it][0m
Training Epoch: 3/5, step 3/4 completed (loss: 0.3539196252822876): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.23s/it][0m
Training Epoch: 3/5, step 3/4 completed (loss: 0.31459495425224304): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.23s/it][0m
Training Epoch: 3/5, step 3/4 completed (loss: 0.33240947127342224): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.23s/it][0m
Max CUDA memory allocated was 18 GB
Max CUDA memory reserved was 20 GB
Peak active CUDA memory was 18 GB
CUDA Malloc retries : 201
CPU Total Peak Memory consumed during the train (max): 2 GB
evaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.87s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.91s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.07s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.07s/it][0m
 eval_ppl=tensor(1.8260, device='cuda:0') eval_epoch_loss=tensor(0.6021, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory
best eval loss on epoch 3 is 0.6021366715431213
Epoch 3: train_perplexity=1.6027, train_epoch_loss=0.4717, epoch time 54.3528296100003s
Training Epoch: 4:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 4:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 4:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 4:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training Epoch: 4:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.6260057091712952):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.7343907952308655):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.25098681449890137):  25%|[34m██████▎                  [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.9253420829772949):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.10s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.7343907952308655):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.3090027868747711):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.40s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.25098681449890137):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.41s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.3419366478919983):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.41s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.6260057091712952):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.41s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.2825499475002289):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.41s/it][0mTraining Epoch: 4/5, step 0/4 completed (loss: 0.9253420829772949):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.41s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.3288358747959137):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.41s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.2825499475002289):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.3419366478919983):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.2966817617416382):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.8468821048736572):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.3288358747959137):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.3500208556652069):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 1/4 completed (loss: 0.3090027868747711):  75%|[34m███████████████████▌      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.41578903794288635):  75%|[34m██████████████████▊      [0m| 3/4 [00:39<00:13, 13.19s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.2966817617416382): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 3/4 completed (loss: 0.29064008593559265): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.8468821048736572): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 3/4 completed (loss: 0.27490419149398804): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.3500208556652069): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 3/4 completed (loss: 0.25454768538475037): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 2/4 completed (loss: 0.41578903794288635): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 3/4 completed (loss: 0.31888705492019653): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.08s/it][0mTraining Epoch: 4/5, step 3/4 completed (loss: 0.29064008593559265): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.22s/it][0m
Training Epoch: 4/5, step 3/4 completed (loss: 0.27490419149398804): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.22s/it][0m
Training Epoch: 4/5, step 3/4 completed (loss: 0.25454768538475037): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.22s/it][0m
Training Epoch: 4/5, step 3/4 completed (loss: 0.31888705492019653): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.23s/it][0m
Max CUDA memory allocated was 18 GB
Max CUDA memory reserved was 20 GB
Peak active CUDA memory was 18 GB
CUDA Malloc retries : 301
CPU Total Peak Memory consumed during the train (max): 2 GB
evaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.03s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.06s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.07s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:03<00:00,  3.07s/it][0m
 eval_ppl=tensor(1.8138, device='cuda:0') eval_epoch_loss=tensor(0.5954, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory
best eval loss on epoch 4 is 0.5954346060752869
Epoch 4: train_perplexity=1.5341, train_epoch_loss=0.4280, epoch time 54.470950840000114s
Training Epoch: 5:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 5:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 5:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mTraining Epoch: 5:   0%|[34m                                                                                   [0m| 0/4 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training Epoch: 5:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.16s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.6199930906295776):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.16s/it][0mTraining Epoch: 5:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.17s/it][0mTraining Epoch: 5:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.16s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.5919836163520813):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.17s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.8768705129623413):  25%|[34m██████▌                   [0m| 1/4 [00:14<00:42, 14.16s/it][0mTraining Epoch: 5:  25%|[34m██████████████████▊                                                        [0m| 1/4 [00:14<00:42, 14.17s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.22243016958236694):  25%|[34m██████▎                  [0m| 1/4 [00:14<00:42, 14.17s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.5919836163520813):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.2532603442668915):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.8768705129623413):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.29433155059814453):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.22243016958236694):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.3120999336242676):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 0/4 completed (loss: 0.6199930906295776):  50%|[34m█████████████             [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.27583184838294983):  50%|[34m████████████▌            [0m| 2/4 [00:27<00:26, 13.43s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.2532603442668915):  75%|[34m███████████████████▌      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.27583184838294983):  75%|[34m██████████████████▊      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.267657607793808):  75%|[34m████████████████████▎      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.3120999336242676):  75%|[34m███████████████████▌      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.3773525059223175):  75%|[34m███████████████████▌      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 1/4 completed (loss: 0.29433155059814453):  75%|[34m██████████████████▊      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.823844850063324):  75%|[34m████████████████████▎      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.3086210787296295):  75%|[34m███████████████████▌      [0m| 3/4 [00:40<00:13, 13.20s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.3773525059223175): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.3086210787296295): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 3/4 completed (loss: 0.2911509871482849): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 3/4 completed (loss: 0.22708991169929504): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.267657607793808): 100%|[34m███████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 3/4 completed (loss: 0.25950366258621216): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 2/4 completed (loss: 0.823844850063324): 100%|[34m███████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 3/4 completed (loss: 0.2444223314523697): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.09s/it][0mTraining Epoch: 5/5, step 3/4 completed (loss: 0.25950366258621216): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.24s/it][0m
Training Epoch: 5/5, step 3/4 completed (loss: 0.2911509871482849): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.24s/it][0m
Training Epoch: 5/5, step 3/4 completed (loss: 0.2444223314523697): 100%|[34m██████████████████████████[0m| 4/4 [00:52<00:00, 13.24s/it][0m
Training Epoch: 5/5, step 3/4 completed (loss: 0.22708991169929504): 100%|[34m█████████████████████████[0m| 4/4 [00:52<00:00, 13.24s/it][0m
Max CUDA memory allocated was 18 GB
Max CUDA memory reserved was 20 GB
Peak active CUDA memory was 18 GB
CUDA Malloc retries : 401
CPU Total Peak Memory consumed during the train (max): 2 GB
evaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mevaluating Epoch:   0%|[32m                                                                                    [0m| 0/1 [00:00<?, ?it/s][0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.87s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.87s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.87s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.87s/it][0mevaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.90s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.90s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.90s/it][0m
evaluating Epoch: 100%|[32m████████████████████████████████████████████████████████████████████████████[0m| 1/1 [00:02<00:00,  2.91s/it][0m
 eval_ppl=tensor(1.8061, device='cuda:0') eval_epoch_loss=tensor(0.5912, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory
best eval loss on epoch 5 is 0.5911798477172852
Epoch 5: train_perplexity=1.4776, train_epoch_loss=0.3904, epoch time 54.39945154599991s
INFO:root:Training process complete
INFO:root:Training process complete
INFO:root:Training process complete
INFO:root:Training process complete
INFO:root:Key: avg_train_prep, Value: 1.667337131500244
INFO:root:Key: avg_train_loss, Value: 0.5053526163101196
INFO:root:Key: avg_eval_prep, Value: 1.848797607421875
INFO:root:Key: avg_eval_loss, Value: 0.6140999674797059
INFO:root:Key: avg_epoch_time, Value: 54.921607062400106
INFO:root:Key: avg_checkpoint_time, Value: 7.924248780000198
INFO:root:Combining pre-trained base model with the PEFT adapter module.
Loading checkpoint shards:   0%|                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████████████████▎                                            | 1/3 [00:00<00:00,  2.71it/s]Loading checkpoint shards:  67%|████████████████████████████████████████████▋                      | 2/3 [00:00<00:00,  2.71it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  3.02it/s]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.93it/s]
INFO:root:Saving the combined model in safetensors format.
INFO:root:Saving complete.
INFO:root:Saving the tokenizer.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
INFO:root:Saving complete.
INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole
Uploaded CodeLlama-7b-Python-HF_run1_2024-04-05-05-41-25.tar.gz to S3 bucket s3://sandbox-dump/codellama-test/finetuned_models
Success notification sent via email!
