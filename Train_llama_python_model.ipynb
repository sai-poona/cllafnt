{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4bbee3f-2e95-4e28-8eba-b3663e32debf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.28.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting appdirs==1.4.4 (from -r requirements.txt (line 2))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting bitsandbytes==0.43.0 (from -r requirements.txt (line 3))\n",
      "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting datasets==2.18.0 (from -r requirements.txt (line 4))\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting fire==0.6.0 (from -r requirements.txt (line 5))\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio==4.23.0 (from -r requirements.txt (line 6))\n",
      "  Downloading gradio-4.23.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting loralib==0.1.2 (from -r requirements.txt (line 7))\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: matplotlib==3.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (3.8.3)\n",
      "Collecting optimum==1.18.0 (from -r requirements.txt (line 9))\n",
      "  Downloading optimum-1.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting peft==0.10.0 (from -r requirements.txt (line 10))\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting py7zr==0.21.0 (from -r requirements.txt (line 11))\n",
      "  Downloading py7zr-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (2.6.1)\n",
      "Collecting torch==2.2.1 (from -r requirements.txt (line 13))\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers==4.39.0 (from -r requirements.txt (line 14))\n",
      "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.12.0)\n",
      "Collecting sentencepiece==0.2.0 (from -r requirements.txt (line 16))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 1)) (6.0.1)\n",
      "Collecting huggingface-hub (from accelerate==0.28.0->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate==0.28.0->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (4.66.2)\n",
      "Collecting xxhash (from datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 4)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 4)) (2024.2.0)\n",
      "Collecting aiohttp (from datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fire==0.6.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fire==0.6.0->-r requirements.txt (line 5)) (2.4.0)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting fastapi (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading fastapi-0.110.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.14.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading gradio_client-0.14.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio==4.23.0->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio==4.23.0->-r requirements.txt (line 6)) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio==4.23.0->-r requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio==4.23.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "Collecting orjson~=3.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio==4.23.0->-r requirements.txt (line 6)) (10.2.0)\n",
      "Collecting pydub (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading typer-0.12.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio==4.23.0->-r requirements.txt (line 6)) (4.9.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.8.3->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.8.3->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.8.3->-r requirements.txt (line 8)) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.8.3->-r requirements.txt (line 8)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.8.3->-r requirements.txt (line 8)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.8.3->-r requirements.txt (line 8)) (2.8.2)\n",
      "Collecting coloredlogs (from optimum==1.18.0->-r requirements.txt (line 9))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from optimum==1.18.0->-r requirements.txt (line 9)) (1.12)\n",
      "Collecting texttable (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.16.0 (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.15.9 (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr==0.21.0->-r requirements.txt (line 11))\n",
      "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from py7zr==0.21.0->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch==2.2.1->-r requirements.txt (line 13)) (3.2.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.39.0->-r requirements.txt (line 14))\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers==4.39.0->-r requirements.txt (line 14))\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.14.0->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->-r requirements.txt (line 13))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 12)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 12)) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.23.0->-r requirements.txt (line 6)) (4.21.1)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 4)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 4))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (4.3.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (0.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0->-r requirements.txt (line 4)) (2.0.7)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum==1.18.0->-r requirements.txt (line 9)) (4.25.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6)) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "\u001b[33mWARNING: typer 0.12.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.18.0->-r requirements.txt (line 9))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->optimum==1.18.0->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.23.0->-r requirements.txt (line 6)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.23.0->-r requirements.txt (line 6)) (0.33.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.23.0->-r requirements.txt (line 6)) (0.18.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6)) (2.17.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio==4.23.0->-r requirements.txt (line 6)) (1.2.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.23.0->-r requirements.txt (line 6))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio-4.23.0-py3-none-any.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Downloading optimum-1.18.0-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.9/409.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.14.0-py3-none-any.whl (312 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m605.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m932.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: fire, ffmpy\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=121ab8bec1ab79d58a314b24279504de24d00a6fbc56a3bb7b20485db4fa9304\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=698968f62c3d388f501a1bd4abcb8e18612a5473c6a9972c846c657a57b25d91\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
      "Successfully built fire ffmpy\n",
      "Installing collected packages: texttable, sentencepiece, pydub, ffmpy, appdirs, xxhash, websockets, uvicorn, triton, toolz, tomlkit, shellingham, semantic-version, safetensors, ruff, regex, pyzstd, python-multipart, pyppmd, pycryptodomex, pybcj, pyarrow-hotfix, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multivolumefile, multidict, mdurl, loralib, inflate64, humanfriendly, frozenlist, fire, async-timeout, aiofiles, yarl, starlette, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, huggingface-hub, coloredlogs, aiosignal, tokenizers, rich, nvidia-cusolver-cu12, gradio-client, fastapi, aiohttp, typer, transformers, torch, altair, datasets, bitsandbytes, accelerate, peft, optimum, gradio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0\n",
      "    Uninstalling torch-2.1.0:\n",
      "      Successfully uninstalled torch-2.1.0\n",
      "Successfully installed accelerate-0.28.0 aiofiles-23.2.1 aiohttp-3.9.3 aiosignal-1.3.1 altair-5.3.0 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.43.0 coloredlogs-15.0.1 datasets-2.18.0 fastapi-0.110.1 ffmpy-0.3.2 fire-0.6.0 frozenlist-1.4.1 gradio-4.23.0 gradio-client-0.14.0 huggingface-hub-0.22.2 humanfriendly-10.0 inflate64-1.0.0 loralib-0.1.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multivolumefile-0.2.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 optimum-1.18.0 orjson-3.10.0 peft-0.10.0 py7zr-0.21.0 pyarrow-hotfix-0.6 pybcj-1.0.2 pycryptodomex-3.20.0 pydub-0.25.1 pyppmd-1.1.0 python-multipart-0.0.9 pyzstd-0.15.10 regex-2023.12.25 rich-13.7.1 ruff-0.3.5 safetensors-0.4.2 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 starlette-0.37.2 texttable-1.7.0 tokenizers-0.15.2 tomlkit-0.12.0 toolz-0.12.1 torch-2.2.1 transformers-4.39.0 triton-2.2.0 typer-0.12.1 uvicorn-0.29.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d7b41-6c97-4c0e-8984-6a5927e81b7d",
   "metadata": {},
   "source": [
    "### Data processing from CSV to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9664600e-b161-42be-a020-38a3ab53c20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"Dataset_refined.csv\")\n",
    "\n",
    "def df_to_jsonl_shuffle(df, filename):\n",
    "    shuffled_df = df.sample(frac=1)  # Shuffle the DataFrame\n",
    "    with open(filename, 'w') as file:\n",
    "        for index, row in shuffled_df.iterrows():\n",
    "            json_obj = row.to_json()\n",
    "            file.write(json_obj + '\\n')\n",
    "\n",
    "def split_train_test(df, train_frac=0.9):\n",
    "    train_size = int(len(df) * train_frac)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "    return train_df, test_df\n",
    "\n",
    "# Split DataFrame into training and test sets\n",
    "train_df, test_df = split_train_test(df, train_frac=0.9)\n",
    "\n",
    "# Convert training DataFrame to JSONL\n",
    "df_to_jsonl_shuffle(train_df, 'data/train/train.jsonl')\n",
    "\n",
    "# Convert test DataFrame to JSONL\n",
    "df_to_jsonl_shuffle(test_df, 'data/validation/test.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f401831-b768-46d0-9600-6e0a54a536ff",
   "metadata": {},
   "source": [
    "### prompt template creation and Prompt completion testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe31d3e-85f2-4e5f-967e-efa9d3fc80a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = {\n",
    "    \"java_context\": \"Below code demonstrates how to perform bisecting k-means clustering and evaluate the clustering performance using Spark MLlib in Java. Initially, a SparkSession is created. Then, the code loads a dataset in LIBSVM format using Spark's read method. Subsequently, a BisectingKMeans model is trained on the dataset with a specified number of clusters (K) and a seed value for reproducibility. Predictions are made on the dataset using the trained model, and the clustering quality is evaluated using the Silhouette score. Finally, the cluster centers are displayed, providing insights into the characteristics of each cluster. \",\n",
    "    \"java\": \"```java\\npackage org.apache.spark.examples.ml;\\n\\n// $example on$\\nimport org.apache.spark.ml.clustering.BisectingKMeans;\\nimport org.apache.spark.ml.clustering.BisectingKMeansModel;\\nimport org.apache.spark.ml.evaluation.ClusteringEvaluator;\\nimport org.apache.spark.ml.linalg.Vector;\\nimport org.apache.spark.sql.Dataset;\\nimport org.apache.spark.sql.Row;\\n// $example off$\\nimport org.apache.spark.sql.SparkSession;\\n\\n\\n/**\\n * An example demonstrating bisecting k-means clustering.\\n * Run with\\n * <pre>\\n * bin/run-example ml.JavaBisectingKMeansExample\\n * </pre>\\n */\\npublic class JavaBisectingKMeansExample {\\n\\n  public static void main(String[] args) {\\n    SparkSession spark = SparkSession\\n      .builder()\\n      .appName(\\\"JavaBisectingKMeansExample\\\")\\n      .getOrCreate();\\n\\n    // $example on$\\n    // Loads data.\\n    Dataset<Row> dataset = spark.read().format(\\\"libsvm\\\").load(\\\"data/mllib/sample_kmeans_data.txt\\\");\\n\\n    // Trains a bisecting k-means model.\\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\\n    BisectingKMeansModel model = bkm.fit(dataset);\\n\\n    // Make predictions\\n    Dataset<Row> predictions = model.transform(dataset);\\n\\n    // Evaluate clustering by computing Silhouette score\\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\\n\\n    double silhouette = evaluator.evaluate(predictions);\\n    System.out.println(\\\"Silhouette with squared euclidean distance = \\\" + silhouette);\\n\\n    // Shows the result.\\n    System.out.println(\\\"Cluster Centers: \\\");\\n    Vector[] centers = model.clusterCenters();\\n    for (Vector center : centers) {\\n      System.out.println(center);\\n    }\\n    // $example off$\\n\\n    spark.stop();\\n  }\\n}```\",\n",
    "    \"python_context\": \"Python equivalent code which demonstrates how to perform bisecting k-means clustering and evaluate the clustering performance utilizes PySpark's MLlib library. Firstly, a SparkSession is initialized. Then, the code loads a dataset in LIBSVM format using Spark's read method. Subsequently, a BisectingKMeans model is trained on the dataset with a specified number of clusters (K) and a seed value for reproducibility. Predictions are made on the dataset using the trained model, and the clustering quality is evaluated using the Silhouette score. Finally, the cluster centers are displayed, offering insights into the characteristics of each cluster. \",\n",
    "    \"python\": \"```python\\nfrom pyspark.ml.clustering import BisectingKMeans\\nfrom pyspark.ml.evaluation import ClusteringEvaluator\\n# $example off$\\nfrom pyspark.sql import SparkSession\\n\\nif __name__ == \\\"__main__\\\":\\n    spark = SparkSession\\\\\\n        .builder\\\\\\n        .appName(\\\"BisectingKMeansExample\\\")\\\\\\n        .getOrCreate()\\n\\n    # $example on$\\n    # Loads data.\\n    dataset = spark.read.format(\\\"libsvm\\\").load(\\\"data/mllib/sample_kmeans_data.txt\\\")\\n\\n    # Trains a bisecting k-means model.\\n    bkm = BisectingKMeans().setK(2).setSeed(1)\\n    model = bkm.fit(dataset)\\n\\n    # Make predictions\\n    predictions = model.transform(dataset)\\n\\n    # Evaluate clustering by computing Silhouette score\\n    evaluator = ClusteringEvaluator()\\n\\n    silhouette = evaluator.evaluate(predictions)\\n    print(\\\"Silhouette with squared euclidean distance = \\\" + str(silhouette))\\n\\n    # Shows the result.\\n    print(\\\"Cluster Centers: \\\")\\n    centers = model.clusterCenters()\\n    for center in centers:\\n        print(center)\\n    # $example off$\\n\\n    spark.stop()```\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39c4e9b-9717-4749-95d2-650c2fb8954d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"\"\"Below is an instruction that describes a problem and its code implementation in Java. Write a response which converts the Java implementation to an implementation in Python.\n",
    "Problem Context:\n",
    "{java_context}\n",
    "Code in Java:\n",
    "{java}\n",
    "\"\"\",\n",
    "    \"completion\": \"\"\" \n",
    "Solution Context:\n",
    "{python_context}\n",
    "\n",
    "Code in Python:\n",
    "{python}\n",
    "\"\"\",\n",
    "}\n",
    "with open(\"./data/template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f067c10d-0b5e-4d79-84a4-65f281fca15c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Below is an instruction that describes a problem and its code implementation in Java. Write a response which converts the Java implementation to an implementation in Python.\\n\\nProblem Context:\\n{java_context}\\n\\nCode in Java:\\n{java}\\n',\n",
       " 'completion': ' \\nSolution Context:\\n{python_context}\\n                        \\nCode in Python:\\n{python}\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file\n",
    "with open('./data/template.json', 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now you can access the data as a dictionary\n",
    "# For example, if your JSON file has a key named 'name', you can access it like this:\n",
    "name = data\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2afe650c-abf0-4f14-bfb5-a9cd8845721f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\n\\nBelow is an instruction that describes a problem and its code implementation in Java. Write a response which converts the Java implementation to an implementation in Python.\\n\\nProblem Context:\\n{java_context}\\n\\nCode in Java:\\n{java}\\n\\n\\n### Response:\\n\\n \\nSolution Context:\\n{python_context}\\n                        \\nCode in Python:\\n{python}\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = f\"\"\"### Instruction:\n",
    "\n",
    "{template['prompt']}\n",
    "\n",
    "### Response:\n",
    "\n",
    "{template['completion']}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c586594-81b7-437e-82ab-c572f4726d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\n\\nBelow is an instruction that describes a problem and its code implementation in Java. Write a response which converts the Java implementation to an implementation in Python.\\n\\nProblem Context:\\nBelow code demonstrates how to perform bisecting k-means clustering and evaluate the clustering performance using Spark MLlib in Java. Initially, a SparkSession is created. Then, the code loads a dataset in LIBSVM format using Spark\\'s read method. Subsequently, a BisectingKMeans model is trained on the dataset with a specified number of clusters (K) and a seed value for reproducibility. Predictions are made on the dataset using the trained model, and the clustering quality is evaluated using the Silhouette score. Finally, the cluster centers are displayed, providing insights into the characteristics of each cluster. \\n\\nCode in Java:\\n```java\\npackage org.apache.spark.examples.ml;\\n\\n// $example on$\\nimport org.apache.spark.ml.clustering.BisectingKMeans;\\nimport org.apache.spark.ml.clustering.BisectingKMeansModel;\\nimport org.apache.spark.ml.evaluation.ClusteringEvaluator;\\nimport org.apache.spark.ml.linalg.Vector;\\nimport org.apache.spark.sql.Dataset;\\nimport org.apache.spark.sql.Row;\\n// $example off$\\nimport org.apache.spark.sql.SparkSession;\\n\\n\\n/**\\n * An example demonstrating bisecting k-means clustering.\\n * Run with\\n * <pre>\\n * bin/run-example ml.JavaBisectingKMeansExample\\n * </pre>\\n */\\npublic class JavaBisectingKMeansExample {\\n\\n  public static void main(String[] args) {\\n    SparkSession spark = SparkSession\\n      .builder()\\n      .appName(\"JavaBisectingKMeansExample\")\\n      .getOrCreate();\\n\\n    // $example on$\\n    // Loads data.\\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\\n\\n    // Trains a bisecting k-means model.\\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\\n    BisectingKMeansModel model = bkm.fit(dataset);\\n\\n    // Make predictions\\n    Dataset<Row> predictions = model.transform(dataset);\\n\\n    // Evaluate clustering by computing Silhouette score\\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\\n\\n    double silhouette = evaluator.evaluate(predictions);\\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\\n\\n    // Shows the result.\\n    System.out.println(\"Cluster Centers: \");\\n    Vector[] centers = model.clusterCenters();\\n    for (Vector center : centers) {\\n      System.out.println(center);\\n    }\\n    // $example off$\\n\\n    spark.stop();\\n  }\\n}```\\n\\n\\n### Response:\\n\\n \\nSolution Context:\\nPython equivalent code which demonstrates how to perform bisecting k-means clustering and evaluate the clustering performance utilizes PySpark\\'s MLlib library. Firstly, a SparkSession is initialized. Then, the code loads a dataset in LIBSVM format using Spark\\'s read method. Subsequently, a BisectingKMeans model is trained on the dataset with a specified number of clusters (K) and a seed value for reproducibility. Predictions are made on the dataset using the trained model, and the clustering quality is evaluated using the Silhouette score. Finally, the cluster centers are displayed, offering insights into the characteristics of each cluster. \\n                        \\nCode in Python:\\n```python\\nfrom pyspark.ml.clustering import BisectingKMeans\\nfrom pyspark.ml.evaluation import ClusteringEvaluator\\n# $example off$\\nfrom pyspark.sql import SparkSession\\n\\nif __name__ == \"__main__\":\\n    spark = SparkSession\\\\\\n        .builder\\\\\\n        .appName(\"BisectingKMeansExample\")\\\\\\n        .getOrCreate()\\n\\n    # $example on$\\n    # Loads data.\\n    dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\\n\\n    # Trains a bisecting k-means model.\\n    bkm = BisectingKMeans().setK(2).setSeed(1)\\n    model = bkm.fit(dataset)\\n\\n    # Make predictions\\n    predictions = model.transform(dataset)\\n\\n    # Evaluate clustering by computing Silhouette score\\n    evaluator = ClusteringEvaluator()\\n\\n    silhouette = evaluator.evaluate(predictions)\\n    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\\n\\n    # Shows the result.\\n    print(\"Cluster Centers: \")\\n    centers = model.clusterCenters()\\n    for center in centers:\\n        print(center)\\n    # $example off$\\n\\n    spark.stop()```\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = prompt_template.format(**x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fa381-2d91-402e-98d0-bc5634028aeb",
   "metadata": {},
   "source": [
    "Note to self: \n",
    "- Loading a 13B Code Llama model requires instances bigger than g5.12xlarge and g4dn.12xlarge\n",
    "- Both have 4 GPU's 96GB GPU memory, 48vCPU's and 192GB CPU memory (RAM)\n",
    "- When loading the 13B pretrained model, the RAM is not sufficient. Need a bigger instance\n",
    "- If I force the model to load using GPU, by setting device_map=\"cuda\" in AutoModelForCausalLM.from_pretrained, I'm hitting GPU OOM errors\n",
    "    -  CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 47.06 MiB is free. \n",
    "- Using a 13B model for testing seems to be greedy. Switching to 7B models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8ca6a-85e4-4646-8143-1b45db44d52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finetuning Args: Namespace(model_dir='./models/CodeLlama-7b-Python-HF', per_device_train_batch_size=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=1, num_workers_dataloader=1, learning_rate=0.001, weight_decay=0.0, gamma=0.85, seed=10, int8_quantization=False, freeze_layers=False, num_freeze_layers=1, use_fast_kernels=False, save_metrics=False, run_validation=True, val_batch_size=1, enable_fsdp=True, fsdp_checkpoint_root_dir='./checkpoints/CodeLlama-7b-Python-HF', low_cpu_fsdp=False, mixed_precision=True, use_fp16=False, pure_bf16=False, optimizer='AdamW', save_optimizer=False, use_peft=True, peft_method='lora', peft_output_dir='./output/CodeLlama-7b-Python-HF', lora_r=8, lora_alpha=32, lora_dropout=0.05, target_modules='q_proj,v_proj', train_dir='./data/vivek/train', validation_dir='./data/vivek/validation', file_extension='jsonl', prompt_template='./data/vivek/template.json', validation_split_ratio=0.2, max_input_length=-1, model_output_dir='finetuned_model')\n",
      "INFO:root:Executing command:\n",
      "INFO:root:['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-7b-Python-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '1', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--train_dir', './data/vivek/train', '--validation_dir', './data/vivek/validation', '--file_extension', 'jsonl', '--prompt_template', './data/vivek/template.json', '--validation_split_ratio', '0.2', '--max_input_length', '-1', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-7b-Python-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-7b-Python-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']\n",
      "[2024-04-03 05:05:02,103] torch.distributed.run: [WARNING] \n",
      "[2024-04-03 05:05:02,103] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-03 05:05:02,103] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-04-03 05:05:02,103] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 0. Rank is 0. World Size is 4\n",
      "INFO:root:Setting torch device = 0\n",
      "INFO:root:Loading the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running with torch dist debug set to detail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 2. Rank is 2. World Size is 4\n",
      "INFO:root:Setting torch device = 2\n",
      "INFO:root:Local rank is 3. Rank is 3. World Size is 4\n",
      "INFO:root:Setting torch device = 3\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 1. Rank is 1. World Size is 4\n",
      "INFO:root:Setting torch device = 1\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Loading the tokenizer.\n",
      "INFO:root:Loading the tokenizer.\n",
      "INFO:root:Loading the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Generating train split: 90 examples [00:00, 469.17 examples/s]\n",
      "Generating validation split: 10 examples [00:00, 7456.54 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 9177.02 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 8199.30 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6824.69 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 934.12 examples/s]\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 678.87 examples/s]\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 632.71 examples/s]\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 589.87 examples/s]\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 594.36 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 3631.43 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1133.57 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 603.17 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 605.56 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 745.93 examples/s]\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 590.83 examples/s]INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 582.28 examples/s]\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:--> Training Set Length = 68\n",
      "INFO:root:--> Validation Set Length = 4\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Python-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [50:22<00:00, 1007.44s/it]\n",
      "INFO:root:Printing Model Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model ./models/CodeLlama-7b-Python-HF\n",
      "\n",
      "--> ./models/CodeLlama-7b-Python-HF has 6738.415616 Million params\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bFloat16 enabled for mixed precision - using bfSixteen policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [50:22<00:00, 1007.52s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [50:22<00:00, 1007.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [50:22<00:00, 1007.50s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/4 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n",
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/4 [00:00<?, ?it/s]INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:root:Starting the training process\n",
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/4 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/4 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch: 1/1, step 3/4 completed (loss: 0.5258502960205078): 100%|\u001b[34m██████████\u001b[0m| 4/4 [00:55<00:00, 13.86s/it]\n",
      "Training Epoch: 1/1, step 3/4 completed (loss: 0.4632633626461029): 100%|\u001b[34m██████████\u001b[0m| 4/4 [00:55<00:00, 13.81s/it]\n",
      "Training Epoch: 1/1, step 3/4 completed (loss: 0.5116773247718811): 100%|\u001b[34m██████████\u001b[0m| 4/4 [00:55<00:00, 13.82s/it]\n",
      "Training Epoch: 1/1, step 3/4 completed (loss: 0.5534397959709167): 100%|\u001b[34m██████████\u001b[0m| 4/4 [00:55<00:00, 13.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 1\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch:   0%|\u001b[32m          \u001b[0m| 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.70s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.86s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:02<00:00,  2.86s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:03<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.9563, device='cuda:0') eval_epoch_loss=tensor(0.6711, device='cuda:0')\n",
      "we are about to save the PEFT modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Python-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT modules are saved in ./output/CodeLlama-7b-Python-HF directory\n",
      "best eval loss on epoch 1 is 0.671068012714386\n",
      "Epoch 1: train_perplexity=2.0012, train_epoch_loss=0.6938, epoch time 56.48963721600012s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Key: avg_train_prep, Value: 2.001217842102051\n",
      "INFO:root:Key: avg_train_loss, Value: 0.6937559843063354\n",
      "INFO:root:Key: avg_eval_prep, Value: 1.9563255310058594\n",
      "INFO:root:Key: avg_eval_loss, Value: 0.671068012714386\n",
      "INFO:root:Key: avg_epoch_time, Value: 56.48963721600012\n",
      "INFO:root:Key: avg_checkpoint_time, Value: 9.73087351400045\n",
      "INFO:root:Combining pre-trained base model with the PEFT adapter module.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.93it/s]\n",
      "INFO:root:Saving the combined model in safetensors format.\n",
      "INFO:root:Saving complete.\n",
      "INFO:root:Saving the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Saving complete.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python train.py \\\n",
    "    --model_dir ./models/CodeLlama-7b-Python-HF \\\n",
    "    --enable_fsdp True \\\n",
    "    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \\\n",
    "    --num_epochs 1 \\\n",
    "    --int8_quantization False \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --seed 10 \\\n",
    "    --use_peft True \\\n",
    "    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\n",
    "    --train_dir ./data/train \\\n",
    "    --validation_dir ./data/validation \\\n",
    "    --file_extension jsonl \\\n",
    "    --prompt_template ./data/template.json\n",
    "    --model_output_dir ./finetuned_model/CodeLlama-7b-Python-HF/run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a33fc7b-db63-4610-a772-299dd6439447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrying the above command to save peft model as I made a mistake of passing incorrect arg parameter for base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4501457-3a90-4885-810c-ceb910341a8f",
   "metadata": {},
   "source": [
    "### Training CodeLlama Python-7b Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184e3668-d1c8-47e6-8ef4-30685a7becdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Finetuning Args: Namespace(model_dir='./models/CodeLlama-7b-Instruct-HF', per_device_train_batch_size=4, batching_strategy='packing', context_length=4096, gradient_accumulation_steps=1, gradient_clipping=False, gradient_clipping_threshold=1.0, num_epochs=5, num_workers_dataloader=1, learning_rate=0.001, weight_decay=0.0, gamma=0.85, seed=10, int8_quantization=False, freeze_layers=False, num_freeze_layers=1, use_fast_kernels=False, save_metrics=False, run_validation=True, val_batch_size=1, enable_fsdp=True, fsdp_checkpoint_root_dir='./checkpoints/CodeLlama-7b-Instruct-HF', low_cpu_fsdp=False, mixed_precision=True, use_fp16=False, pure_bf16=False, optimizer='AdamW', save_optimizer=False, use_peft=True, peft_method='lora', peft_output_dir='./output/CodeLlama-7b-Instruct-HF', lora_r=8, lora_alpha=32, lora_dropout=0.05, target_modules='q_proj,v_proj', train_dir='./data/vivek/train', validation_dir='./data/vivek/validation', file_extension='jsonl', prompt_template='./data/vivek/template.json', validation_split_ratio=0.2, max_input_length=-1, model_output_dir='./finetuned_model/CodeLlama-7b-Instruct-HF/run1')\n",
      "INFO:root:Executing command:\n",
      "INFO:root:['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'finetuning.py', '--num_gpus', '4', '--model_name', './models/CodeLlama-7b-Instruct-HF', '--batch_size_training', '4', '--batching_strategy', 'packing', '--context_length', '4096', '--gradient_accumulation_steps', '1', '--gradient_clipping', 'False', '--gradient_clipping_threshold', '1.0', '--num_epochs', '5', '--num_workers_dataloader', '1', '--lr', '0.001', '--weight_decay', '0.0', '--gamma', '0.85', '--seed', '10', '--freeze_layers', 'False', '--num_freeze_layers', '1', '--use_fast_kernels', 'False', '--save_metrics', 'False', '--run_validation', 'True', '--val_batch_size', '1', '--quantization', 'False', '--train_dir', './data/vivek/train', '--validation_dir', './data/vivek/validation', '--file_extension', 'jsonl', '--prompt_template', './data/vivek/template.json', '--validation_split_ratio', '0.2', '--max_input_length', '-1', '--enable_fsdp', '--dist_checkpoint_root_folder', './checkpoints/CodeLlama-7b-Instruct-HF', '--low_cpu_fsdp', 'False', '--mixed_precision', 'True', '--use_fp16', 'False', '--pure_bf16', 'False', '--optimizer', 'AdamW', '--save_optimizer', 'False', '--use_peft', '--peft_method', 'lora', '--output_dir', './output/CodeLlama-7b-Instruct-HF', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj']\n",
      "[2024-04-07 12:13:33,505] torch.distributed.run: [WARNING] \n",
      "[2024-04-07 12:13:33,505] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-04-07 12:13:33,505] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-04-07 12:13:33,505] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "/home/ec2-user/SageMaker/cllafnt/finetuning.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 1. Rank is 1. World Size is 4\n",
      "INFO:root:Setting torch device = 1\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 2. Rank is 2. World Size is 4\n",
      "INFO:root:Setting torch device = 2\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 3. Rank is 3. World Size is 4\n",
      "INFO:root:Setting torch device = 3\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "INFO:root:Local rank is 0. Rank is 0. World Size is 4\n",
      "INFO:root:Setting torch device = 0\n",
      "INFO:root:Loading the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running with torch dist debug set to detail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Loading the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Loading the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Loading the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:--> Training Set Length = 107\n",
      "INFO:root:--> Validation Set Length = 7\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Instruct-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Instruct-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Using the default value of max_input_length=2048.\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Instruct-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "INFO:root:Loading the pre-trained model and setup its configuration\n",
      "INFO:root:Model Name: ./models/CodeLlama-7b-Instruct-HF\n",
      "INFO:root:enable_fsdp is set to True and low_cpu_fsdp is set to False\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.80s/it]\n",
      "INFO:root:Printing Model Size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model ./models/CodeLlama-7b-Instruct-HF\n",
      "\n",
      "--> ./models/CodeLlama-7b-Instruct-HF has 6738.546688 Million params\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,740,992 || trainable%: 0.06220473254091146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bFloat16 enabled for mixed precision - using bfSixteen policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.81s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.81s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.82s/it]\n",
      "INFO:root:Printing Model Size\n",
      "INFO:root:Using PEFT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,740,992 || trainable%: 0.06220473254091146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,740,992 || trainable%: 0.06220473254091146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,740,992 || trainable%: 0.06220473254091146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up FSDP if enable_fsdp is enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/6 [00:00<?, ?it/s]INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> applying fsdp activation checkpointing...\n",
      "--> applying fsdp activation checkpointing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/6 [00:00<?, ?it/s]INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "INFO:root:Initializing the optimizer and learning rate scheduler\n",
      "INFO:root:Starting the training process\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/6 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1/5, step 5/6 completed (loss: 0.47047489881515503): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:20<00:00, 13.40s/it]\n",
      "Training Epoch: 1/5, step 5/6 completed (loss: 0.534217894077301): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:20<00:00, 13.47s/it]\n",
      "Training Epoch: 1/5, step 5/6 completed (loss: 0.5123025178909302): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:20<00:00, 13.43s/it]\n",
      "Training Epoch: 1/5, step 5/6 completed (loss: 0.4984665513038635): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:20<00:00, 13.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 1\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.88s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.88s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.88s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.7931, device='cuda:0') eval_epoch_loss=tensor(0.5839, device='cuda:0')\n",
      "we are about to save the PEFT modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Instruct-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Instruct-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Instruct-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./models/CodeLlama-7b-Instruct-HF - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT modules are saved in ./output/CodeLlama-7b-Instruct-HF directory\n",
      "best eval loss on epoch 1 is 0.5839325785636902\n",
      "Epoch 1: train_perplexity=1.9117, train_epoch_loss=0.6480, epoch time 81.15726579400143s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/5, step 5/6 completed (loss: 0.40087318420410156): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 2/5, step 5/6 completed (loss: 0.3831484913825989): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 2/5, step 5/6 completed (loss: 0.4410686790943146): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 2/5, step 5/6 completed (loss: 0.41780155897140503): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 101\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.7218, device='cuda:0') eval_epoch_loss=tensor(0.5434, device='cuda:0')\n",
      "we are about to save the PEFT modules\n",
      "PEFT modules are saved in ./output/CodeLlama-7b-Instruct-HF directory\n",
      "best eval loss on epoch 2 is 0.5433874130249023\n",
      "Epoch 2: train_perplexity=1.6324, train_epoch_loss=0.4900, epoch time 78.89981068899942s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/5, step 5/6 completed (loss: 0.39495712518692017): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 3/5, step 5/6 completed (loss: 0.33214330673217773): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 3/5, step 5/6 completed (loss: 0.36514174938201904): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 3/5, step 5/6 completed (loss: 0.3530711531639099): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 201\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.85s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.85s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.85s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.7175, device='cuda:0') eval_epoch_loss=tensor(0.5409, device='cuda:0')\n",
      "we are about to save the PEFT modules\n",
      "PEFT modules are saved in ./output/CodeLlama-7b-Instruct-HF directory\n",
      "best eval loss on epoch 3 is 0.5408512949943542\n",
      "Epoch 3: train_perplexity=1.5396, train_epoch_loss=0.4315, epoch time 78.87803173200155s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4/5, step 5/6 completed (loss: 0.3564712107181549): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it] \n",
      "Training Epoch: 4/5, step 5/6 completed (loss: 0.2951991558074951): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 4/5, step 5/6 completed (loss: 0.32806456089019775): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n",
      "Training Epoch: 4/5, step 5/6 completed (loss: 0.3126378357410431): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 301\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.7169, device='cuda:0') eval_epoch_loss=tensor(0.5405, device='cuda:0')\n",
      "we are about to save the PEFT modules\n",
      "PEFT modules are saved in ./output/CodeLlama-7b-Instruct-HF directory\n",
      "best eval loss on epoch 4 is 0.5405406951904297\n",
      "Epoch 4: train_perplexity=1.4750, train_epoch_loss=0.3886, epoch time 78.8627174860012s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5/5, step 5/6 completed (loss: 0.2932030260562897): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.06s/it]]\n",
      "Training Epoch: 5/5, step 5/6 completed (loss: 0.2609100043773651): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.06s/it]\n",
      "Training Epoch: 5/5, step 5/6 completed (loss: 0.3223790228366852): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.06s/it]\n",
      "Training Epoch: 5/5, step 5/6 completed (loss: 0.27691683173179626): 100%|\u001b[34m██████████\u001b[0m| 6/6 [01:18<00:00, 13.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 18 GB\n",
      "Max CUDA memory reserved was 20 GB\n",
      "Peak active CUDA memory was 18 GB\n",
      "CUDA Malloc retries : 401\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n",
      "\n",
      "evaluating Epoch: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:05<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(1.7366, device='cuda:0') eval_epoch_loss=tensor(0.5520, device='cuda:0')\n",
      "Epoch 5: train_perplexity=1.4203, train_epoch_loss=0.3508, epoch time 78.84463565199985s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Training process complete\n",
      "INFO:root:Key: avg_train_prep, Value: 1.5957965135574341\n",
      "INFO:root:Key: avg_train_loss, Value: 0.46181475520133974\n",
      "INFO:root:Key: avg_eval_prep, Value: 1.7371895790100098\n",
      "INFO:root:Key: avg_eval_loss, Value: 0.5498505353927612\n",
      "INFO:root:Key: avg_epoch_time, Value: 79.32849227060069\n",
      "INFO:root:Key: avg_checkpoint_time, Value: 5.268354036599339\n",
      "INFO:root:Combining pre-trained base model with the PEFT adapter module.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.00it/s]\n",
      "INFO:root:Saving the combined model in safetensors format.\n",
      "INFO:root:Saving complete.\n",
      "INFO:root:Saving the tokenizer.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "INFO:root:Saving complete.\n",
      "INFO:root:Model Tarring process has begun\n",
      "INFO:root:Uploading to S3\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:root:Uploaded CodeLlama-7b-Instruct-HF_run1_2024-04-07-12-21-53.tar.gz to S3 bucket s3://sandbox-dump/codellama-test/finetuned_models\n",
      "INFO:root:Success notification sent via email!\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python train.py \\\n",
    "    --model_dir ./models/CodeLlama-7b-Python-HF \\\n",
    "    --enable_fsdp True \\\n",
    "    --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF \\\n",
    "    --num_epochs 1 \\\n",
    "    --int8_quantization False \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --seed 10 \\\n",
    "    --use_peft True \\\n",
    "    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\n",
    "    --train_dir ./data/train \\\n",
    "    --validation_dir ./data/validation \\\n",
    "    --file_extension jsonl \\\n",
    "    --prompt_template ./data/template.json \\\n",
    "    --model_output_dir ./finetuned_model/CodeLlama-7b-Python-HF/run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054efc75-7f05-4668-a63c-5735e4e000d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup python train.py --model_dir ./models/CodeLlama-7b-Python-HF --enable_fsdp True --fsdp_checkpoint_root_dir ./checkpoints/CodeLlama-7b-Python-HF --num_epochs 5 \\--int8_quantization False \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --seed 10 \\\n",
    "    --use_peft True \\\n",
    "    --peft_output_dir ./output/CodeLlama-7b-Python-HF \\\n",
    "    --train_dir ./data/train \\\n",
    "    --validation_dir ./data/validation \\\n",
    "    --file_extension jsonl \\\n",
    "    --prompt_template ./data/template.json \\\n",
    "    --model_output_dir ./finetuned_model/CodeLlama-7b-Python-HF/run1 &"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
